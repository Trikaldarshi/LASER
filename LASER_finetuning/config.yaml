runner:
  total_steps: 3600
  gradient_clipping: 1
  gradient_accumulate_steps: 8
  log_step: 100
  eval_step: 3601
  save_step: 100
  max_keep: 1
  freeze_layers: True
  baseline: custom # "superb" for benchmark or "custom" for custom
  
optimizer:
  name: AdamW
  lr: 2.0e-5

# comment the whole scheduler config block
# to disable learning rate scheduling
scheduler:
  name: linear_schedule_with_warmup
  num_warmup_steps: 1000

downstream_expert:
  datarc:
    path: path_to_libri # dir structure: path_to_libri/LibriSpeech/train-clean-100
    num_workers: 2
    train_batch_size: 1
    test_base_path: path to LASER finetuned model

  modelrc:
    model_name: hubert
    input_dim: 256
    loss_type: softdtw_lav # "softdtw_lav" or "softdtw"
    sigma: 20
    margin: 2
    gamma: 0.1
    alpha: 1000
